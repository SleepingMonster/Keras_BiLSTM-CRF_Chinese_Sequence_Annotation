{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow\n",
    "import keras\n",
    "import pickle\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Masking, Dense, Input, TimeDistributed, Activation, Lambda, Dropout\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参量\n",
    "SAVE_PATH = 'config.pkl'\n",
    "BiRNN_UNITS = 200\n",
    "BATCH_SIZE = 16  # 32、64\n",
    "EMBED_DIM=300\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  “  人们  常  说  生活  是  一  部  教科书  ，  而  血  与  火  ...\n",
      "1  “  心  静  渐  知  春  似  海  ，  花  深  每  觉  影  生  香  。\n",
      "2    “  吃  屎  的  东西  ，  连  一  捆  麦  也  铡  不  动  呀  ？\n",
      "3  他  “  严格要求  自己  ，  从  一个  科举  出身  的  进士  成为  一...\n",
      "4  “  征  而  未  用  的  耕地  和  有  收益  的  土地  ，  不准  ...\n",
      "                                                   0\n",
      "0                      扬帆  远东  做  与  中国  合作  的  先行  \n",
      "1                            希腊  的  经济  结构  较  特殊  。\n",
      "2  海运  业  雄踞  全球  之  首  ，  按  吨位  计  占  世界  总数  的...\n",
      "3  另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业...\n",
      "4  多年来  ，  中  希  贸易  始终  处于  较低  的  水平  ，  希腊  几乎...\n"
     ]
    }
   ],
   "source": [
    "# train_set = open('msrseg/msr_training.utf8', 'r', encoding='utf-8')\n",
    "train_set = pd.read_csv('msrseg/msr_training.utf8', encoding= 'utf8', header=None)  # 不把第一行作为列属性，且pd读出来就是数据帧，就是字符串\n",
    "test_set = pd.read_csv('msrseg/msr_test_gold.utf8', encoding='utf8', header=None)\n",
    "print(train_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子转换成字序列\n",
    "def get_char(sentence):\n",
    "    char_list = []\n",
    "    sentence = ''.join(sentence.split('  ')) #去掉空格\n",
    "    for i in sentence:\n",
    "        char_list.append(i)\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将句子转成BMES序列\n",
    "def get_label(sentence):\n",
    "    result = []\n",
    "    word_list = sentence.split('  ')  #两个空格来分隔一个词\n",
    "    for i in range(len(word_list)):\n",
    "        if len(word_list[i]) == 1:\n",
    "            result.append('S')\n",
    "        elif len(word_list[i]) == 2:\n",
    "            result.append('B')\n",
    "            result.append('E')\n",
    "        else:\n",
    "            temp = len(word_list[i]) - 2\n",
    "            result.append('B')\n",
    "            result.extend('M'*temp)\n",
    "            result.append('E')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    char, content, label = [], [], []\n",
    "    maxlen = 0\n",
    "\n",
    "    for i in range(len(file)):  # 记得加range！！\n",
    "        line = file.loc[i,0]   # 用loc来访问dataframe\n",
    "        line = line.strip('\\n') #去掉换行符\n",
    "        line = line.strip(' ')  #去掉开头和结尾的空格\n",
    "        \n",
    "        char_list = get_char(line)        #获得字列表\n",
    "        label_list = get_label(line)      # 获得标签列表\n",
    "        maxlen = max(maxlen, len(char_list))\n",
    "        if len(char_list)!=len(label_list):\n",
    "            continue   # 由于数据集的问题，所以要删掉有问题的样本（在训练集中有26个样本；测试集中无）\n",
    "        char.extend(char_list)            #每一个单元是1个字\n",
    "        content.append(char_list)         # 每一个单元是一行里面的各个字（分好）\n",
    "        label.append(label_list)          #每一个单元是一行里面打好标签的结果（含标点）\n",
    "    return char, content, label, maxlen  #word是单列表，content和label是双层列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data: padding\n",
    "def process_data(char_list, label_list, vocab, chunk_tags, MAXLEN):\n",
    "    # idx2vocab = {idx: char for idx, char in enumerate(vocab)}\n",
    "    vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    # get every char of every word, map to idx in vocab, set to <UNK> if not in vocab\n",
    "    x = [[vocab2idx.get(char, 1) for char in s] for s in char_list]\n",
    "    # map label to idx\n",
    "    y_chunk = [[chunk_tags.index(label) for label in s] for s in label_list]\n",
    "    # padding of x, default is 0(symbolizes <PAD>). padding includes:over->cutoff, less->padding. default: left_padding\n",
    "    x = pad_sequences(x, maxlen=MAXLEN, value=0)\n",
    "    # padding of y_chunk\n",
    "    y_chunk = pad_sequences(y_chunk, maxlen=MAXLEN, value=-1)\n",
    "    # one_hot:\n",
    "    y_chunk = to_categorical(y_chunk, len(chunk_tags))\n",
    "    # y_chunk = np.eye(len(chunk_tags), dtype='float32')[y_chunk]\n",
    "    return x, y_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    chunk_tags = ['S','B','M','E']\n",
    "    train_char, train_content, train_label, _ = read_file(train_set)\n",
    "    test_char, test_content, test_label, maxlen = read_file(test_set)\n",
    "    \n",
    "    vocab = list(set(train_char + test_char))   # 合并，构成大词表\n",
    "    special_chars = ['<PAD>', '<UNK>']   #特殊词表示：PAD表示padding，UNK表示词表中没有\n",
    "    vocab = special_chars + vocab\n",
    "    \n",
    "    # save initial config data\n",
    "    with open(SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump((train_char, chunk_tags), f)\n",
    "    \n",
    "    # process data: padding\n",
    "    print('maxlen is %d' % maxlen)\n",
    "    train_x, train_y = process_data(train_content, train_label, vocab, chunk_tags, maxlen)\n",
    "    test_x, test_y = process_data(test_content, test_label, vocab, chunk_tags, maxlen)\n",
    "    return train_x, train_y, test_x, test_y, vocab, chunk_tags, maxlen, test_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_path = 'sgns.context.word-character.char1-1.bz2'  #词向量位置\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings_matrix(word2vec_model, vocab):\n",
    "    char2vec_dict = {}    # 字对词向量\n",
    "    vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    for char, vector in zip(word2vec_model.vocab, word2vec_model.vectors):\n",
    "        char2vec_dict[char] = vector\n",
    "    embeddings_matrix = np.zeros((len(vocab), EMBED_DIM))# form huge matrix\n",
    "    for i in tqdm(range(2, len(vocab))):\n",
    "        char = vocab[i]\n",
    "        if char in char2vec_dict.keys():    # 如果char在词向量列表中，更新权重；否则，赋值为全0（默认）\n",
    "            char_vector = char2vec_dict[char]\n",
    "            embeddings_matrix[i] = char_vector\n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen is 308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5179/5179 [00:00<00:00, 243736.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 308)               0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 308)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 308, 300)          1554300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 308, 200)          320800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 308, 200)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 308, 4)            804       \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 308, 4)            44        \n",
      "=================================================================\n",
      "Total params: 1,875,948\n",
      "Trainable params: 1,875,948\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# K.clear_session()\n",
    "train_x, train_y, test_x, test_y, vocab, chunk_tags, maxlen, test_content = load_data()\n",
    "embeddings_matrix = make_embeddings_matrix(word2vec_model, vocab)\n",
    "# input layer\n",
    "inputs = Input(shape=(maxlen, ), dtype='int32')\n",
    "# masking layer 屏蔽层\n",
    "x = Masking(mask_value=0)(inputs)\n",
    "# embedding layer: map the word to it's weights(with embedding-matrix)\n",
    "x = Embedding(len(vocab), EMBED_DIM, weights=[embeddings_matrix], input_length=maxlen, trainable=True)(x)\n",
    "# Bi-LSTM layer\n",
    "x = Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True))(x)\n",
    "# Dropout: 正则化，防止过拟合.argument means percentage\n",
    "x = Dropout(0.5)(x)\n",
    "# 一维展开，全连接\n",
    "x = TimeDistributed(Dense(len(chunk_tags)))(x)\n",
    "# output layer\n",
    "outputs = CRF(len(chunk_tags))(x)\n",
    "# model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# print arguments of each layer\n",
    "model.summary()\n",
    "# target_function: includes optimizer, function_type, metrics\n",
    "# SGD = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "SGD = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "SGDM = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=False)\n",
    "RMSprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "model.compile(optimizer='adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_split=0.1)\n",
    "# test_predict = model.predict(test_x)\n",
    "score = model.evaluate(test_x, test_y, batch_size=BATCH_SIZE)\n",
    "print(score)\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "model.load_weights('model.h5')\n",
    "test_predict = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = [[np.argmax(char) for char in sample] for sample in test_predict]  # get the max label_id\n",
    "test_predict_tag = [[chunk_tags[i] for i in sample ]for sample in test_predict]   # get the label of predic\n",
    "test_gold = [[np.argmax(char) for char in sample] for sample in test_y]  # get the label_id\n",
    "test_gold_tag = [[chunk_tags[i] for i in sample] for sample in test_gold]  # get the label of real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_content, _, _ = read_file(test_set)\n",
    "f_sum = 0   # 各个句子的f值之和\n",
    "for i in range(len(test_predict_tag)):\n",
    "    correct_word_num = 0  # 分对词数\n",
    "    predict_word_num = 0  # 预测分词结果的总词数\n",
    "    gold_word_num = 0     # 实际分词结果总词数\n",
    "    predict_sample = test_predict_tag[i]\n",
    "    gold_sample = test_gold_tag[i]\n",
    "    s_len = len(test_content[i])   # the real length of the sentence\n",
    "    flag = False   # true: inside a word; false: outside a word\n",
    "    for j in range(len(predict_sample) - s_len, len(predict_sample)):\n",
    "        if gold_sample[j] == 'S' or gold_sample[j] == 'E' or j == len(predict_sample) - 1:   # update gold_word_num\n",
    "            gold_word_num += 1\n",
    "        if predict_sample[j] == 'S' or predict_sample[j] == 'E' or j == len(predict_sample) - 1:   # update predict_word_num\n",
    "            predict_word_num += 1\n",
    "        if gold_sample[j] != predict_sample[j]:\n",
    "            flag = False\n",
    "            continue\n",
    "        elif gold_sample[j] == predict_sample[j] and (gold_sample[j] == 'S' or (gold_sample[j] == 'E' and flag is True)):\n",
    "            correct_word_num += 1\n",
    "            flag = False\n",
    "        elif gold_sample[j] == predict_sample[j] and gold_sample[j] == 'B':\n",
    "            flag = True   # inside the word: start\n",
    "    precision = float(correct_word_num) /float( predict_word_num)\n",
    "    recall = float(correct_word_num) / float(gold_word_num)\n",
    "    if precision == 0 and recall == 0: f1 = 0\n",
    "    else: f1 = 2 * precision * recall / (precision + recall)\n",
    "    f_sum += f1\n",
    "print(f_sum / len(test_predict_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['扬帆  远东  做  与  中国  合作  的  先行', '希腊  的  经济  结构  较  特殊  。', '海运  业  雄踞  全球  之  首  ，  按  吨  位计  占  世界  总数  的  １７％  。', '另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业  规模  相对  较小  。', '多年来  ，  中  希  贸易  始终  处于  较低  的  水平  ，  希腊  几乎  没有  在  中国  投资  。']\n"
     ]
    }
   ],
   "source": [
    "test_result = []\n",
    "vocab2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "_, test_content, _, _ = read_file(test_set)\n",
    "for i in range(len(test_predict)):\n",
    "# for i in range(1):\n",
    "    sentence = ''\n",
    "    s_len = len(test_content[i])\n",
    "    sample = test_predict_tag[i]\n",
    "    for j in range(s_len):\n",
    "        idx = len(sample)- s_len + j\n",
    "        if sample[idx]=='B' or sample[idx]=='M' or j==s_len-1:\n",
    "            sentence = sentence + test_content[i][j]\n",
    "        else:\n",
    "            sentence = sentence + test_content[i][j]\n",
    "            sentence = sentence + '  '\n",
    "    test_result.append(sentence)\n",
    "print(test_result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('msrseg/msr_test_predict.txt', 'w')\n",
    "file1 = open('msrseg/msr_test_predict.utf8', 'w', encoding='utf-8')   # 当不是对应位置的参数时，要加上参数名！\n",
    "for i in range(len(test_result)):\n",
    "    file.write(test_result[i])\n",
    "    file1.write(test_result[i])\n",
    "    file.write('\\n')\n",
    "    file1.write('\\n')\n",
    "file.close()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                       0\n",
      "0                           扬帆  远东  做  与  中国  合作  的  先行\n",
      "1                               希腊  的  经济  结构  较  特殊  。\n",
      "2     海运  业  雄踞  全球  之  首  ，  按  吨  位计  占  世界  总数  的...\n",
      "3     另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业...\n",
      "4     多年来  ，  中  希  贸易  始终  处于  较低  的  水平  ，  希腊  几乎...\n",
      "...                                                 ...\n",
      "3980  而  计算机  造词  功能  的  运用  ，  形成  了  一种  被  称为  “ ...\n",
      "3981  至于  计算机  的  使用  对  学生  书法  、  笔顺  、  发音  的  影响...\n",
      "3982  再  往远  些  看  ，  随着  汉字  识别  和  语音  识别  技术  的  ...\n",
      "3983  而  计算机  翻译  系统  与  现代  汉语  分析  相辅  相成  ，  正  推...\n",
      "3984  每  跨  过  一  道  障碍  ，  人们  都  会  发现  一片  新  的  ...\n",
      "\n",
      "[3985 rows x 1 columns]>\n"
     ]
    }
   ],
   "source": [
    "test_predict = pd.read_csv('msrseg/msr_test_predict_10.utf8', encoding= 'utf8', header=None)  # 不把第一行作为列属性，且pd读出来就是数据帧，就是字符串\n",
    "print(test_predict.head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
